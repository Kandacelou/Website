[
  {
    "objectID": "textanalysis.html",
    "href": "textanalysis.html",
    "title": "The Office Lines Analysis",
    "section": "",
    "text": "# load in packages\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(knitr)\n\n# import data set\noffice_lines &lt;- read.csv(\"the-office_lines.csv\")\n\n# filter out Jim's lines when he says \"Pam\" in every season\nJim_lines &lt;- office_lines |&gt;\n  filter(Character == \"Jim\") |&gt;\n  filter(str_detect(Line, \"\\\\bPam[a-z]*\\\\b\")) |&gt;\n  group_by(Season, Episode_Number) |&gt;\n  summarize(n = n(), .groups = \"drop\") \n\n# plot of Jim's frequency of the word \"Pam\"\nggplot(Jim_lines, aes(x = Episode_Number, y = n, fill = Season)) +\n  geom_col() +\n  labs(\n    x = \"Episode Number\",\n    y = \"# of 'Pam' Occurrences\", \n    title = \"How Many Times Jim Said 'Pam' in 'The Office'\"\n  )\n\n\n\n\n\n\n\n\n“The Office,” although heavily referred to as a “comedy,” has one of the most popular romantic couples in television sitcom history: Jim and Pam. In this analysis, I counted how many times Jim said the name “Pam” / “Pamela” per episode in every season. The graph is also color-coded by season: getting a lighter shade of blue as the seasons progress. Based on the data, Jim said the name “Pam” the most on episode 4 collectively throughout every season, and this trend would decrease the longer a season went on. Ultimately, Jim referenced “Pam” in 123 episodes throughout the course of the TV show.\n\n# detect whenever somebody laughs\nlaugh_counts &lt;- office_lines |&gt;\n  filter(str_detect\n         (str_to_lower\n           (str_trim(Line)), \"\\\\[laughing\\\\]|\\\\[laughs\\\\]\")) |&gt;\n  group_by(Episode_Number, Season) |&gt;\n  summarize(n = n()) |&gt;\n  arrange(desc(n)) |&gt;\n  head(n = 10)\n\n# create a table indicating laugh counts\nkable(laugh_counts, caption = \"Top 10 Laugh Counts by Season and Episode\")\n\n\nTop 10 Laugh Counts by Season and Episode\n\n\nEpisode_Number\nSeason\nn\n\n\n\n\n10\n8\n13\n\n\n18\n9\n13\n\n\n6\n1\n11\n\n\n23\n3\n11\n\n\n23\n9\n11\n\n\n5\n9\n8\n\n\n13\n7\n8\n\n\n1\n1\n7\n\n\n4\n1\n7\n\n\n16\n9\n7\n\n\n\n\n\n“The Office” is well known for their improvisations and breaking of character, leading to genuine laughter adding on to the already scripted occurrences. The above table shows a list of the Top 10 episodes in which characters laughed during their lines. This displays how Season 8, Episode 10 and Season 9, Episode 18 have the most laughter within the episode, at a total of 13 laughs. Surprisingly, these episodes actually do not parallel with what fans think are the “funniest episodes.” ScreenRant ranked Season 2, Episode 12 as the funniest episode and Collider ranked Season 4, Episode 13 the funniest."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kandace Loualhati",
    "section": "",
    "text": "Hi! My name is Kandace Loualhati (Loo-uhl-haw-tee). I’m currently a second year at Pomona College who’s intending on double majoring in Politics and Cognitive Science (with a computational concentration). I’m also a coffee addict (whoops!), an avid adventurer, and have my Seal of Biliteracy in American Sign Language :)"
  },
  {
    "objectID": "calfiredamage.html",
    "href": "calfiredamage.html",
    "title": "California Fire Damage",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readr)\n calfire_damage &lt;- read.csv(\"calfire_damage.csv\")\nggplot(calfire_damage, aes(x = year, y = structures)) +\n  geom_point(color = \"orange\") +\n  labs(\n    x = \"Year\",\n    y = \"Numbers of Structures Destroyed\",\n    title = \"California Fire Damage Per Year\",\n  )\n\n\n\n\n\n\n\n\nCode from: tidytuesday/data/2018/2018-08-21/calfire_damage.csv at master · rfordatascience/tidytuesday (github.com)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hi! My name is Kandace Loualhati (Loo-uhl-haw-tee). I’m currently a second year at Pomona College who’s intending on double majoring in Politics and Cognitive Science (with a computational concentration). I’m also a coffee addict (whoops!), an avid adventurer, and have my Seal of Biliteracy in American Sign Language :)"
  },
  {
    "objectID": "employedwomen.html",
    "href": "employedwomen.html",
    "title": "Employed Women",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readr)\nemployed_gender &lt;- read.csv(\"employed_gender.csv\")\nggplot(employed_gender, aes(x = year, y = full_time_female)) +\n  geom_point(color = \"hotpink\") +\n  labs(\n    x = \"Year\",\n    y = \"% Of Women Employed Full-Time\",\n    title = \"Women Employed Full-Time Per Year\",\n  )\n\n\n\n\n\n\n\n\nCode from: tidytuesday/data/2019/2019-03-05/employed_gender.csv at master · rfordatascience/tidytuesday (github.com)"
  },
  {
    "objectID": "Permutationtest.html",
    "href": "Permutationtest.html",
    "title": "Permutation Test",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(knitr)\n\n\nset.seed(47)\n\n# import data set\ncancer_data &lt;- read.csv(\"wdbc.data\", header = FALSE)\n\n# find proportion of malignant tumors in women with breast cancer\nmal_prop &lt;- cancer_data |&gt;\n  summarize(proportion = mean(V2 == \"M\")) |&gt;\n  pull()\nmal_prop\n\n[1] 0.3725835\n\n# write a function to simulate women who are equally likely to have malignant and benign tumors\ntotal_women &lt;- nrow(cancer_data)\nrandom_tum &lt;- function(rep, total_women) {\n  result = sample(c(\"M\", \"B\"), size = total_women,\n                 replace = TRUE, prob = c(0.5, 0.5))\n  return(mean(result == \"M\"))\n}\n\n# repeat the function many times\nmap_dbl(1:20, random_tum, total_women)\n\n [1] 0.5465729 0.4674868 0.4956063 0.4692443 0.4833040 0.4797891 0.5166960\n [8] 0.4885764 0.5096661 0.5043937 0.4745167 0.4991213 0.5149385 0.5026362\n[15] 0.5008787 0.4938489 0.4920914 0.5289982 0.5289982 0.5026362\n\n# find p value \nnum_exper &lt;- 5000\nrandom_map &lt;- map_dbl(1:num_exper, random_tum, total_women)\nsum(random_map &gt;= mal_prop) / num_exper\n\n[1] 1"
  }
]